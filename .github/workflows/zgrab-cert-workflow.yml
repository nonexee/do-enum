name: ZGrab2 Certificate Collection

on:
  workflow_dispatch:
    inputs:
      start_line:
        description: 'Start processing from line number (1-based)'
        required: true
        type: number
      batch_size:
        description: 'Number of ranges to process'
        required: true
        type: number
        default: 3

jobs:
  collect-certs:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          persist-credentials: true
          fetch-depth: 0

      - name: Configure Git
        run: |
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git config --global user.name "github-actions[bot]"
          git fetch origin
          git reset --hard origin/main

      - name: Setup workspace
        run: |
          mkdir -p cert_results
          chmod -R 777 "${{ github.workspace }}"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq gzip

      - name: Process ranges with ZGrab2
        run: |
          split_and_compress_results() {
            local input_file="$1"
            local results_dir="$2"
            local chunk_size=$((90 * 1024 * 1024))  # 2MB in bytes
            
            # Get total certificates and metadata
            local total_count=$(jq '.total_count' "$input_file")
            local scan_time=$(jq -r '.scan_time' "$input_file")
            local file_size=$(stat -f%z "$input_file" 2>/dev/null || stat -c%s "$input_file")
            local size_mb=$((file_size / 1024 / 1024))
            
            echo "Processing file of size ${size_mb}MB..."
            
            if [ $size_mb -gt 90 ]; then
                echo "Splitting into 2MB chunks..."
                
                # Calculate certificates per chunk based on file size ratio
                local certs_per_chunk=$((total_count * chunk_size / file_size))
                local chunk_number=0
                
                # Create metadata file
                jq '{
                    total_count: .total_count,
                    scan_time: .scan_time,
                    split: true,
                    original_size_mb: '$size_mb',
                    chunks: []
                }' "$input_file" > "${results_dir}/certs.json"
                
                # Split and process chunks
                jq -c '.certificates | _nwise('$certs_per_chunk')' "$input_file" | while read -r chunk; do
                    chunk_number=$((chunk_number + 1))
                    local chunk_file="${results_dir}/certs_part_${chunk_number}.json"
                    local chunk_gz="${results_dir}/certs_part_${chunk_number}.json.gz"
                    
                    # Create chunk JSON
                    echo "{
                        \"certificates\": ${chunk},
                        \"part\": ${chunk_number},
                        \"scan_time\": \"${scan_time}\"
                    }" | jq '.' > "$chunk_file"
                    
                    # Compress chunk
                    gzip -9 -c "$chunk_file" > "$chunk_gz"
                    rm -f "$chunk_file"
                    
                    # Update metadata with chunk info
                    local chunk_size=$(stat -f%z "$chunk_gz" 2>/dev/null || stat -c%s "$chunk_gz")
                    jq --arg part "${chunk_number}" \
                       --arg size "$((chunk_size / 1024))KB" \
                    '.chunks += [{
                        part: $part,
                        compressed_size: $size
                    }]' "${results_dir}/certs.json" > "${results_dir}/certs.json.tmp"
                    mv "${results_dir}/certs.json.tmp" "${results_dir}/certs.json"
                done
            else
                echo "Compressing as single file..."
                gzip -9 -c "$input_file" > "${results_dir}/certs.json.gz"
                jq '{
                    total_count: .total_count,
                    scan_time: .scan_time,
                    split: false,
                    compressed_size: "'$(($(stat -f%z "${results_dir}/certs.json.gz" 2>/dev/null || stat -c%s "${results_dir}/certs.json.gz") / 1024))'KB"
                }' "$input_file" > "${results_dir}/certs.json"
            fi
          }

          process_zgrab_output() {
            local input_file="$1"
            local results_dir="$2"
            
            if [ ! -s "$input_file" ]; then
              echo "Empty or missing input file: $input_file"
              echo '{
                "certificates": [],
                "total_count": 0,
                "scan_time": "'"$(date -u +"%Y-%m-%dT%H:%M:%SZ")"'"
              }' > "${results_dir}/certs.json"
              return
            fi
            
            echo "Processing certificates..."
            local temp_json="${results_dir}/temp_certs.json"
            
            # Process and create initial JSON
            jq -c '[inputs | select(.data != null and .data.tls != null) |
              select(.data.tls.result.handshake_log.server_certificates.certificate.parsed.subject.common_name[0] != null) |
              select(.data.tls.result.handshake_log.server_certificates.certificate.parsed.subject.common_name[0] != "") | {
                ip: .ip,
                tls: {
                  protocol_version: (.data.tls.result.handshake_log.server_hello.version.name // null),
                  cipher_suite: {
                    name: (.data.tls.result.handshake_log.server_hello.cipher_suite.name // null),
                    value: (.data.tls.result.handshake_log.server_hello.cipher_suite.value // null)
                  },
                  certificate_details: {
                    validity: {
                      start: (.data.tls.result.handshake_log.server_certificates.certificate.parsed.validity.start // null),
                      end: (.data.tls.result.handshake_log.server_certificates.certificate.parsed.validity.end // null)
                    },
                    issuer: {
                      common_name: (.data.tls.result.handshake_log.server_certificates.certificate.parsed.issuer.common_name[0] // null),
                      organization: (.data.tls.result.handshake_log.server_certificates.certificate.parsed.issuer.organization[0] // null),
                      country: (.data.tls.result.handshake_log.server_certificates.certificate.parsed.issuer.country[0] // null)
                    },
                    subject: {
                      common_name: (.data.tls.result.handshake_log.server_certificates.certificate.parsed.subject.common_name[0] // null)
                    },
                    public_key: {
                      algorithm: (.data.tls.result.handshake_log.server_certificates.certificate.parsed.subject_key_info.key_algorithm.name // null),
                      length: (.data.tls.result.handshake_log.server_certificates.certificate.parsed.subject_key_info.rsa_public_key.length // null)
                    }
                  },
                  security_audit: {
                    certificate_chain: [
                      .data.tls.result.handshake_log.server_certificates.chain[]? | 
                      select(. != null) | {
                        issuer_common_name: (.parsed.issuer.common_name[0] // null),
                        valid: (.parsed.signature.valid // false)
                      }
                    ],
                    browser_trusted: (.data.tls.result.handshake_log.server_certificates.validation.browser_trusted // false)
                  }
                }
              }] | {
                certificates: .,
                total_count: length,
                scan_time: "'"$(date -u +"%Y-%m-%dT%H:%M:%SZ")"'"
              }' "$input_file" > "$temp_json"

            # Split and compress results
            split_and_compress_results "$temp_json" "$results_dir"
            rm -f "$temp_json"
          }

          START_LINE=${{ inputs.start_line }}
          END_LINE=$((START_LINE + ${{ inputs.batch_size }} - 1))
          
          while IFS= read -r cidr; do
            [ -z "$cidr" ] && continue
            
            echo "Processing range: $cidr"
            safe_name=$(echo "$cidr" | tr '/' '_')
            results_dir="cert_results/${safe_name}"
            mkdir -p "$results_dir"
            
            # Check if ZMap results exist
            zmap_result="zmap_results/${safe_name}.json"
            if [ ! -f "$zmap_result" ]; then
              echo "No ZMap results found for $cidr, skipping..."
              continue
            fi
            
            # Extract IPs from ZMap results
            jq -r '.ips[]?' "$zmap_result" > "tmp_ips.txt" 2>/dev/null
            
            if [ -s tmp_ips.txt ]; then
              echo "Running ZGrab2 for ${safe_name}..."
              
              if docker run --rm --network=host -v "${{ github.workspace }}":/data \
                sec32/zgrab2 zgrab2 tls --port 443 \
                --timeout 10s \
                --input-file=/data/tmp_ips.txt \
                --output-file=/data/tmp_certs.json; then
                
                process_zgrab_output "tmp_certs.json" "$results_dir"
                
                if [ -f "${results_dir}/certs.json" ]; then
                  total_certs=$(jq '.total_count' "${results_dir}/certs.json")
                  is_split=$(jq -r '.split // false' "${results_dir}/certs.json")
                  
                  # Create summary file
                  if [ "$is_split" = "true" ]; then
                    num_chunks=$(jq '.chunks | length' "${results_dir}/certs.json")
                    echo "{
                      \"range\": \"$cidr\",
                      \"total_certificates\": $total_certs,
                      \"split\": true,
                      \"total_chunks\": $num_chunks,
                      \"scan_time\": \"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\"
                    }" > "${results_dir}/summary.json"
                    echo "Processed $total_certs certificates in $num_chunks chunks for $cidr"
                  else
                    echo "{
                      \"range\": \"$cidr\",
                      \"total_certificates\": $total_certs,
                      \"split\": false,
                      \"scan_time\": \"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\"
                    }" > "${results_dir}/summary.json"
                    echo "Processed $total_certs certificates for $cidr"
                  fi
                  
                  # Commit and push results
                  git add "${results_dir}/"
                  if git commit -m "Add certificate scan results for $cidr"; then
                    for i in {1..3}; do
                      if git push origin main; then
                        break
                      else
                        echo "Push failed, attempt $i of 3. Pulling latest changes..."
                        git pull --rebase origin main
                      fi
                    done
                  fi
                else
                  echo "Failed to create output file for $cidr"
                fi
              else
                echo "ZGrab2 scan failed for range: $cidr"
              fi
            else
              echo "No IPs found in ZMap results for range: $cidr"
            fi
            
            # Cleanup temporary files
            rm -f tmp_ips.txt tmp_certs.json
          done < <(awk -v start="$START_LINE" -v end="$END_LINE" 'NR >= start && NR <= end' do_ranges.txt)
